# -*- coding: utf-8 -*-
"""gradientTape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O6qMrXCAMfNRDq0hQqP6_eYPQ8kBP4fS
"""

# -*- coding: utf-8 -*-
# Renato Santos Aranha Jan/21

# ===================
# packages importing
# ===================

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from imutils import paths
import numpy as np
import cv2
import time
import sys
from google.colab import drive

# Mounting google drive (requires authentication)
drive.mount('/content/drive', force_remount=True)

# ===================
# Preparing the data
# ===================

def get_data(imPaths):
  print("[INFO] loading images...")
  # grab the list of images in dataset directory
  imagePaths = list(paths.list_images(imPaths))
  data = []
  labels = []
  # loop over the image path
  for imagePath in imagePaths:
    # extract class label from filename
    label = int(imagePath.split(".")[-2][-1])
    # load image, swap color channels, and resize it to 224x224
    image = cv2.imread(imagePath)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (224, 224))
    # update data and labels
    data.append(image)
    labels.append(label)
  # convert data and labels to NumPy arrays while scaling pixel
  # intensities to range [0, 1]
  d = np.array(data) / 255.0
  l = np.array(labels)
  print("[INFO] ok")
  return d, l

def onehot(labels):
  # perform one-hot encoding on labels
  lb = LabelBinarizer()
  labels = lb.fit_transform(labels)
  labels = to_categorical(labels)
  return labels


def split(data, labels, test_size=0.20, stratify=0):
  # partition data into train and test splits using 80% for training
  X = data
  y = labels
  if stratify:
    (trainX, testX, trainY, testY) = train_test_split(X, y, test_size=test_size, stratify=labels, random_state=42)
  else:
    (trainX, testX, trainY, testY) = train_test_split(X, y, test_size=test_size, random_state=42)
  return (trainX, testX, trainY, testY)

# =========================================
# defining tasks (Montgomery and Shenzhen)
# =========================================

# load images
data1, labels1 = get_data('/content/drive/My Drive/Colab Notebooks/mont_CXR_png/')
labels1 = onehot(labels1)
(train1X, test1X, train1Y, test1Y) = split(data1, labels1, test_size=0.20)

data2, labels2 = get_data('/content/drive/My Drive/Colab Notebooks/shen_CXR_png/')
labels2 = onehot(labels2)
(train2X, test2X, train2Y, test2Y) = split(data2, labels2, test_size=0.20)

# define tasks
task1 = [train1X, train1Y, test1X, test1Y]
task2 = [train2X, train2Y, test2X, test2Y]

# ========================
# visualize sample images
# ========================

# print shapes
print(train1X.shape)
print(test1X.shape)
print("=====")
print(train2X.shape)
print(test2X.shape)

# plot samples
plt.imshow(data2[0])

# ======================================================================
# create a new class (keras Model as superclass) to apply EWC loss term
# ======================================================================
# ref: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit

class ModelEWC(Model):
    def __init__(self, inputs, outputs, fisher_dict, optpar_dict):
        super(ModelEWC, self).__init__(inputs, outputs)
        self.fisher_dict = fisher_dict
        self.optpar_dict = optpar_dict

    def get_ewc_loss(self):
        # fisher and optimized parameters are from previous task
        list_args = []
        ewc_term = 0

        # loop over parameter names and calculate ewc loss term
        for i, j in enumerate(self.optpar_dict.keys()):
          params = self.trainable_weights
          arg = (0.5) * self.fisher_dict[j] * (params[i] - self.optpar_dict[j])**2
          list_args.append(arg)

        for i in list_args:
          ewc_term += tf.math.reduce_sum(i)
        return ewc_term
        
    def train_step(self, data):
        # this function is called by .fit() method
        # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)
            ewc_term = self.get_ewc_loss()
            loss += ewc_term

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

# ======================================================
# build model on top of DenseNet pretrained architecure
# ======================================================
# ref: https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/

def build_model(nr_classes=2, shape=(224, 224, 3)):
  # load the DenseNet network, ensuring the head FC layer sets are left off
  base = DenseNet121(weights="imagenet", include_top=False, input_tensor=Input(shape=shape))
  
  # construct the head to be placed on top of the base model
  head_FC = base.output
  head_FC = MaxPooling2D(pool_size=(2, 2))(head_FC)
  head_FC = Flatten(name="flatten")(head_FC)
  head_FC = Dense(64, activation="relu")(head_FC)
  head_FC = Dropout(0.5)(head_FC)
  head_FC = Dense(nr_classes, activation="softmax")(head_FC)
  
  # place head FC model on top of base model
  model = Model(inputs=base.input, outputs=head_FC)

  # freeze parameters in base model
  for layer in base.layers:
    layer.trainable = False
  return model

# ====================================
# defining custom training functions
# ====================================

def on_task_update(task, model, BS):
  loss_history = []
    
  trainX, trainY, _, _ = task

  assert BS <= trainX.shape[0], "batch size should be less than training size"
 
  numUpdates = int(trainX.shape[0] / BS)

  for i in range(0, numUpdates):
    # determine starting and ending slice indexes for the current batch
    start = i * BS
    end = start + BS
    # take a step
    with tf.GradientTape() as tape:
    # make a prediction using model and calculate the loss
      pred = model(trainX[start:end])
      loss_batch = categorical_crossentropy(trainY[start:end], pred)

    loss_history.append(loss_batch)
    # get gradients using tape and update model weights
    params = model.trainable_variables
    grads = tape.gradient(loss_batch, params)
    opt.apply_gradients(zip(grads, params))


  loss = np.mean(loss_history)

  # saving optimized parameters and fisher information of each parameter
  param_names = [i.name for i in params]

  optpar_dict = dict(zip(param_names, params))
  fisher_dict = dict(zip(param_names, grads))
  fisher_dict = {k: pow(v,2) for k,v in fisher_dict.items()}

  return fisher_dict, optpar_dict, model, loss


def train_normal(task, model, BS, EPOCHS):

  loss_history = []

  for epoch in range(0, EPOCHS):
    print("[INFO] epoch {}/{} ".format(epoch + 1, EPOCHS), end="")
    sys.stdout.flush()
    epochStart = time.time()

    # loop over data in batch size increments, and take grads, params and model
    fisher_dict, optpar_dict, modl, loss = on_task_update(task, model, BS)
    model = modl

    loss_history.append(loss)

    epochEnd = time.time()
    elapsed = (epochEnd - epochStart) / 60.0
    print("- loss {} - {:.4} mins".format(loss, elapsed))
  
  print("===============".format(loss, elapsed))

  
  return fisher_dict, optpar_dict, model, loss_history


def get_ewc_loss(fisher_dict, optpar_dict, model, lambd):
  # fisher and optimized parameters are from previous task
  list_args = []
  ewc_term = 0

  # loop over parameter names and calculate ewc loss term
  for i,j in enumerate(optpar_dict.keys()):
    params = model.trainable_weights
    arg = lambd * (0.5) * fisher_dict[j] * (params[i] - optpar_dict[j])**2
    list_args.append(arg)

  for i in list_args:
    ewc_term += tf.math.reduce_sum(i)
  return ewc_term


def feedforwardstep_ewc(X, y, fisher_dict, optpar_dict, model, lambd):
  # keep track of gradients
  with tf.GradientTape() as tape:
    # make a prediction using model and calculate the loss
    pred = model(X)
    loss = categorical_crossentropy(y, pred)
 
    ewc_term = get_ewc_loss(fisher_dict, optpar_dict, model, lambd)
    loss += ewc_term
  
  # calculate gradients using tape and update model weights
  parameters = model.trainable_variables
  grads = tape.gradient(loss, parameters)
  opt.apply_gradients(zip(grads, parameters))

  return model, loss


def train_ewc(task, model, BS, EPOCHS, fisher_dict, optpar_dict, lambd):

  trainX, trainY, _, _ = task
  numUpdates = int(trainX.shape[0] / BS)
  loss_epoch_list = []
  
  for epoch in range(0, EPOCHS):
    print("[INFO] epoch {}/{} ".format(epoch + 1, EPOCHS), end="")
    sys.stdout.flush()
    epochStart = time.time()
    
    loss_history = []
    
    for i in range(0, numUpdates):
      
      # determine starting and ending slice indexes for the current batch
      start = i * BS
      end = start + BS
      # take a step
      model, loss_batch = feedforwardstep_ewc(trainX[start:end],
                                        trainY[start:end],
                                        fisher_dict,
                                        optpar_dict,
                                        model,
                                        1)
      loss_history.append(loss_batch)
    
    loss_epoch_list.append(np.mean(loss_history))

    epochEnd = time.time()
    elapsed = (epochEnd - epochStart) / 60.0
    print("- loss {} - {:.4} mins".format(np.mean(loss_history), elapsed))

  return model, loss_epoch_list

# =============================
# set training hyperparameters
# =============================

# number of epochs, batch size and learning rate
EPOCHS = 10
BS = 16
LR = 1e-3

# build model and initialize optimizer
modl = build_model()
opt = Adam(lr=LR, decay=LR / EPOCHS)

# ====================================================================
# calculate fisher information and optimized parameters for each task
# ====================================================================

fisher_dict1, optpar_dict1, model1, loss_history1 = train_normal(task1, modl, BS, EPOCHS)
fisher_dict2, optpar_dict2, model2, loss_history2 = train_normal(task2, modl, BS, EPOCHS)

# ===============================
# plotting training loss history
# ===============================

plt.clf()
plt.plot(range(0, len(loss_history1)), loss_history1, '-', label='Training loss')
plt.title('Train loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# ===============================
# plotting training loss history
# ===============================

plt.clf()
plt.plot(range(0, len(loss_history2)), loss_history2, '-', label='Training loss')
plt.title('Train loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# ============================
# evaluate model on test data
# ============================

ml = build_model()

# in order to calculate accuracy using Keras' functions we first need to compile the model
ml.compile(metrics=["acc"])
# now that the model is compiled we can compute the accuracy

testX, testY = task1[2], task1[3]
(loss, acc) = ml.evaluate(testX, testY, batch_size=1)
print("[INFO] task1 test accuracy: {:.4f}".format(acc))

testX, testY = task2[2], task2[3]
(loss, acc) = ml.evaluate(testX, testY, batch_size=1)
print("[INFO] task2 test accuracy: {:.4f}".format(acc))

# ============================
# evaluate model on test data
# ============================

# in order to calculate accuracy using Keras' functions we first need to compile the model
model1.compile(metrics=["acc"])
# now that the model is compiled we can compute the accuracy

testX, testY = task1[2], task1[3]
(loss, acc) = model1.evaluate(testX, testY, batch_size=1)
print("[INFO] task1 test accuracy: {:.4f}".format(acc))

testX, testY = task2[2], task2[3]
(loss, acc) = model1.evaluate(testX, testY, batch_size=1)
print("[INFO] task2 test accuracy: {:.4f}".format(acc))

# ============================
# evaluate model on test data
# ============================

# in order to calculate accuracy using Keras' functions we first need to compile the model
model2.compile(metrics=["acc"])
# now that the model is compiled we can compute the accuracy

testX, testY = task1[2], task1[3]
(loss, acc) = model2.evaluate(testX, testY, batch_size=1)
print("[INFO] task1 test accuracy: {:.4f}".format(acc))

testX, testY = task2[2], task2[3]
(loss, acc) = model2.evaluate(testX, testY, batch_size=1)
print("[INFO] task2 test accuracy: {:.4f}".format(acc))

modelo, loss_historico = train_ewc(task1, model2, BS, EPOCHS, fisher_dict2, optpar_dict2, 1)

# ============================
# evaluate model on test data
# ============================

# in order to calculate accuracy using Keras' functions we first need to compile the model
modelo.compile(metrics=["acc"])
# now that the model is compiled we can compute the accuracy

testX, testY = task1[2], task1[3]
(loss, acc) = modelo.evaluate(testX, testY, batch_size=1)
print("[INFO] task1 test accuracy: {:.4f}".format(acc))

testX, testY = task2[2], task2[3]
(loss, acc) = modelo.evaluate(testX, testY, batch_size=1)
print("[INFO] task2 test accuracy: {:.4f}".format(acc))

# ===============================================
# instantiate reference model and ModelEWC class
# ===============================================

baseModel = build_model()

EWC = ModelEWC(inputs=baseModel.input,
               outputs=baseModel.output,
               fisher_dict=fisher_dict2, 
               optpar_dict=optpar_dict2)


# ============================
# compile and train EWC model
# ============================

EWC.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["acc"])

history = EWC.fit(x=train1X, y=train1Y, epochs=5, batch_size=BS)

# =========================
# setting cross validation
# =========================

from sklearn.model_selection import KFold

data = data2
y = labels2

k_fold = KFold(n_splits=3, shuffle=True, random_state=42)

# for train, test in kfold.split(data, y):
# 	print('train: %s, test: %s' % (data[train].shape, data[test].shape))
 
fold_losses = []
cvsacc = []

for k, (train, test) in enumerate(k_fold.split(data, y)):
    print("[fold {0}]".format(k))

    # tsk = [data[train], y[train], data[test], y[test]]
    
    baseModel = build_model()

    EWC = ModelEWC(inputs=baseModel.input,
                   outputs=baseModel.output,
                   fisher_dict=fisher_dict1, 
                   optpar_dict=optpar_dict1)

    EWC.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["acc", tf.keras.metrics.TrueNegatives()])
    EWC.fit(x=data[train], y=y[train], epochs=5, batch_size=BS)
    
    (loss, acc, TN) = EWC.evaluate(data[test], y[test], batch_size=1)
    fold_losses.append(loss)
    cvsacc.append(acc)